import streamlit as st
from genie.testbed import load
from langchain_community.document_loaders import JSONLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
import tempfile, os, json, uuid

# --- UI Setup ---
st.set_page_config(page_title="Chat with R1 Routing Table", page_icon="ğŸ›£ï¸")
st.title("ğŸ›£ï¸ Chat with Your R1 Routing Table")
st.markdown("Ask anything about the live routing table retrieved from R1 using pyATS!")

# --- Cached RAG Pipeline Setup ---
def setup_routing_chain():
    # Step 1: Connect to R1 and get routing table
    testbed = load("testbed.yaml")
    device = testbed.devices["R1"]
    print("ğŸ”Œ Connecting to R1...")
    device.connect(log_stdout=True)
    parsed_output = device.parse("show ip route")

    # Step 2: Write JSON to temp file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".json", mode="w") as tmp:
        json.dump(parsed_output, tmp, indent=2)
        tmp_path = tmp.name

    # Step 3: Load into LangChain Documents
    loader = JSONLoader(
        file_path=tmp_path,
        jq_schema='.',  # 1 route per document
        text_content=False
    )
    documents = loader.load()
    os.remove(tmp_path)

    # Step 4: Embed & Split
    embedding = OpenAIEmbeddings(model="text-embedding-3-small")
    splitter = SemanticChunker(embedding)
    chunks = splitter.split_documents(documents)

    # Step 5: Build Chroma vector store
    vector_store = Chroma.from_documents(chunks, embedding)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    # Step 6: Set up RAG chain
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True
    )

    return qa_chain

qa_chain = setup_routing_chain()

# --- Chat Interaction ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

question = st.text_input("ğŸ’¬ Ask a question about R1's routing table:")

if question:
    with st.spinner("Thinking..."):
        response = qa_chain.invoke({
            "question": question,
            "chat_history": st.session_state.chat_history
        })
        st.session_state.chat_history.append((question, response["answer"]))

# --- Display Chat History ---
for user_q, answer in reversed(st.session_state.chat_history):
    st.markdown(f"**ğŸ§‘â€ğŸ’» You:** {user_q}")
    st.markdown(f"**ğŸ¤– R1 Routing Bot:** {answer}")
    st.markdown("---")
