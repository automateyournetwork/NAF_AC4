from genie.testbed import load
from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
import tempfile
import json
import os

# --- Step 1: Connect to device and get parsed JSON ---
testbed = load("testbed.yaml")
device = testbed.devices["R1"]
device.connect(log_stdout=True)
parsed_output = device.parse("show ip route")

# --- Step 2: Write parsed JSON to temp file ---
with tempfile.NamedTemporaryFile(delete=False, suffix=".json", mode="w") as tmp:
    json.dump(parsed_output, tmp, indent=2)
    tmp_path = tmp.name

# --- Step 3: Load JSON into LangChain Documents ---
loader = JSONLoader(
    file_path=tmp_path,
    jq_schema='.',              # Entire JSON object (one doc per interface)
    text_content=False
)

documents = loader.load()
print(f"\nðŸ“„ Loaded {len(documents)} routing documents\n")

# --- SPLIT 1: RecursiveCharacterTextSplitter ---
print("ðŸ”¹ Splitting with RecursiveCharacterTextSplitter")
recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
recursive_chunks = recursive_splitter.split_documents(documents)
print(f"ðŸ”¹ Recursive: {len(recursive_chunks)} chunks")

for i, chunk in enumerate(recursive_chunks[:2]):
    print(f"\nðŸ§© Recursive Chunk {i+1}:\n{chunk.page_content[:300]}...\n")

# --- SPLIT 2: SemanticChunker ---
print("\nðŸ”¸ Splitting with SemanticChunker (OpenAI embeddings)")
embedding = OpenAIEmbeddings(model="text-embedding-3-small")
semantic_splitter = SemanticChunker(embedding)
semantic_chunks = semantic_splitter.split_documents(documents)
print(f"ðŸ”¸ Semantic: {len(semantic_chunks)} chunks")

for i, chunk in enumerate(semantic_chunks[:2]):
    print(f"\nðŸ§  Semantic Chunk {i+1}:\n{chunk.page_content[:300]}...\n")

# --- Cleanup ---
os.remove(tmp_path)
